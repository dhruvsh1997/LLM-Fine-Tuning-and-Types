# -*- coding: utf-8 -*-
"""SmolLM2_135M_Fine_Tuning_FULL_Anime_Dataset_with_Preplexity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X2z4fvHHhAoFQZK7ttsCKBSoVerAOJsw

## Fine-Tune SmolLM-135M-Instruct on Anime Understanding Dataset

This script fine-tunes the SmolLM-135M-Instruct model on the anime-understanding-dataset using full fine-tuning (updating all weights). It is designed to run in Google Colab with detailed comments for clarity.
"""

# ------------------------ Install Required Packages ------------------------
!pip install transformers datasets evaluate nltk rouge_score polars

# Step 2: Import Libraries and Set Up Environment
import torch
import numpy as np
import evaluate
import nltk
from datasets import Dataset, concatenate_datasets
import polars as pl
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments,
    DataCollatorForLanguageModeling
)
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer

nltk.download("punkt")

# ------------------------ Load Model and Tokenizer ------------------------
model_name = "HuggingFaceTB/SmolLM-135M-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float32,  # Full precision for 16 float16
    device_map=None
)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# ------------------------ Load Dataset ------------------------
anime_configs = [
    "chainsawman", "kurokonobasuke", "onepunch", "hellsing", "frieren", "aot",
    "naruto", "dr_stone", "gundam_00", "darling-in-the-franxx",
    "berserk", "evangelion", "onepiece"
]

train_splits, val_splits = [], []
for cfg in anime_configs:
    df_train = pl.read_ndjson(f"hf://datasets/theblackcat102/anime-understanding-dataset/{cfg}_dev.jsonl").to_pandas()
    df_val = pl.read_ndjson(f"hf://datasets/theblackcat102/anime-understanding-dataset/{cfg}_val.jsonl").to_pandas()
    train_splits.append(Dataset.from_pandas(df_train))
    val_splits.append(Dataset.from_pandas(df_val))

train_dataset = concatenate_datasets(train_splits)
val_dataset = concatenate_datasets(val_splits)

# ------------------------ Format and Tokenize ------------------------
def format_and_tokenize(example):
    question = example['question']
    prompt = (f"Question: {question}\nOptions:\n"
              f"A. {example['A']}\nB. {example['B']}\n"
              f"C. {example['C']}\nD. {example['D']}\nAnswer:")
    correct = example[example['answer']]
    full_text = prompt + " " + correct

    # Tokenize the full text with truncation and explicit padding
    tokens = tokenizer(
        full_text,
        truncation=True,
        max_length=512,
        padding="max_length", # Explicitly pad to max_length
        return_tensors="pt" # Return PyTorch tensors
    )

    # Create labels and set padding tokens to -100
    labels = tokens['input_ids'].clone()
    # Assuming the padding token is set in the tokenizer (which was done in L1Ile8YffB3_)
    # Find where padding occurs and set the corresponding label to -100
    if tokenizer.pad_token_id is not None:
        labels[labels == tokenizer.pad_token_id] = -100

    # Remove batch dimension added by return_tensors="pt" for compatibility with datasets map
    tokens = {key: value.squeeze() for key, value in tokens.items()}

    return tokens

train_dataset = train_dataset.map(format_and_tokenize, remove_columns=train_dataset.column_names)
val_dataset = val_dataset.map(format_and_tokenize, remove_columns=val_dataset.column_names)

# ------------------------ Collator & Metrics ------------------------
collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

def compute_metrics(eval_pred):
    predictions, labels = eval_pred

    # Convert predictions to flat lists
    predictions = np.argmax(predictions, axis=-1) if predictions.ndim == 3 else predictions

    # Replace -100 in labels to pad_token_id so decoding works correctly
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    rouge = evaluate.load("rouge")
    bleu_scores = []
    perplexities = []

    scorer = rouge_scorer.RougeScorer(["rouge1", "rougeL"], use_stemmer=True)
    smoothing = SmoothingFunction().method1

    for ref, pred in zip(decoded_labels, decoded_preds):
        bleu = sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothing)
        bleu_scores.append(bleu)

        # Perplexity: Compute over pred
        inputs = tokenizer(pred, return_tensors="pt").input_ids.to(device)
        with torch.no_grad():
            loss = model(inputs, labels=inputs).loss
            ppl = torch.exp(loss).item()
            perplexities.append(ppl)

    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)
    result["bleu"] = np.mean(bleu_scores)
    result["perplexity"] = np.mean(perplexities)
    return result

# ------------------------ Training ------------------------
training_args = TrainingArguments(
    output_dir="./anime_qa_full_finetune",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=2,
    num_train_epochs=3,
    learning_rate=3e-5,
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=1,
    report_to="none",
    remove_unused_columns=False,
    fp16=False  # Full precision
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

trainer.evaluate()

#### Save the fine-tuned model
trainer.save_model("./smollm_finetuned/final_model")
tokenizer.save_pretrained("./smollm_finetuned/final_model")
print("Fine-tuning completed and model saved")

### Step 9: Test the Fine-Tuned Model

#### Load fine-tuned model and tokenizer
fine_tuned_model = AutoModelForCausalLM.from_pretrained("./smollm_finetuned/final_model", torch_dtype=torch.bfloat16).to(device)
fine_tuned_tokenizer = AutoTokenizer.from_pretrained("./smollm_finetuned/final_model")

#### Test input
test_input = "Instruction: Who is the main character in Chainsaw Man?\n\nResponse:"
inputs = fine_tuned_tokenizer(test_input, return_tensors="pt").to(device)

#### Generate response
outputs = fine_tuned_model.generate(
    inputs["input_ids"],
    max_length=100,
    num_return_sequences=1,
    do_sample=True,
    temperature=0.7
)

#### Decode and print response
response = fine_tuned_tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Model response:")
print(response)





















# ----------- Full Fine-Tuning for Anime QA Dataset with Perplexity, BLEU, and ROUGE Evaluation -----------
# ------------ Install Dependencies --------------
!pip install -q transformers datasets evaluate rouge_score nltk polars

# ------------ Imports -------------------
import torch
import numpy as np
import math
import polars as pl
from datasets import Dataset, concatenate_datasets
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments,
    DataCollatorForLanguageModeling
)
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import nltk
nltk.download('punkt')

# -------- Setup Device and Model ---------------
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "HuggingFaceTB/SmolLM-1.7B-Instruct"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load full-precision model for full fine-tuning
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    # device_map="auto",
    torch_dtype=torch.bfloat16  # use torch.float16 for older GPUs
)

# -------- Load Dataset and Preprocess ----------
anime_configs = [
    "chainsawman", "kurokonobasuke", "onepunch", "hellsing", "frieren", "aot",
    "naruto", "dr_stone", "gundam_00", "darling-in-the-franxx",
    "berserk", "evangelion", "onepiece"
]

train_splits, val_splits = [], []
for cfg in anime_configs:
    df_train = pl.read_ndjson(f"hf://datasets/theblackcat102/anime-understanding-dataset/{cfg}_dev.jsonl").to_pandas()
    df_val = pl.read_ndjson(f"hf://datasets/theblackcat102/anime-understanding-dataset/{cfg}_val.jsonl").to_pandas()
    train_splits.append(Dataset.from_pandas(df_train))
    val_splits.append(Dataset.from_pandas(df_val))

train_dataset = concatenate_datasets(train_splits)
val_dataset = concatenate_datasets(val_splits)

def format_and_tokenize(example):
    question = example['question']
    prompt = (f"Question: {question}\nOptions:\n"
              f"A. {example['A']}\nB. {example['B']}\n"
              f"C. {example['C']}\nD. {example['D']}\nAnswer:")
    correct = example[example['answer']]
    full_text = prompt + " " + correct

    tokens = tokenizer(
        full_text,
        truncation=True,
        padding="max_length",   # âœ… PAD to max_length
        max_length=512,
        return_attention_mask=True,
    )
    tokens["labels"] = tokens["input_ids"].copy()  # Make sure labels are same size
    return tokens


train_dataset = train_dataset.map(format_and_tokenize, remove_columns=train_dataset.column_names)
val_dataset = val_dataset.map(format_and_tokenize, remove_columns=val_dataset.column_names)

collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# -------- TrainingArguments and Trainer --------
args = TrainingArguments(
    output_dir="./anime_qa_full_finetune",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=2,
    num_train_epochs=15,
    learning_rate=2e-5,
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    report_to="none",
    bf16=True,
    remove_unused_columns=True # Changed to True
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=collator
)

# -------- Train -------------------------------
trainer.train()

# -------- Save Model --------------------------
model.save_pretrained("./anime_qa_full_finetuned_model")
tokenizer.save_pretrained("./anime_qa_full_finetuned_model")

# -------- Evaluation Metrics ------------------
scorer = rouge_scorer.RougeScorer(["rouge1", "rouge2", "rougeL"], use_stemmer=True)
smoothing = SmoothingFunction().method1
model.eval()

predictions = []
references = []
rouge1s, rougeLs, bleus, perplexities = [], [], [], []

for example in val_dataset.select(range(15)):
    input_ids = torch.tensor([example['input_ids']]).to(device)
    label_ids = example['labels']
    with torch.no_grad():
        generated_ids = model.generate(input_ids, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)
        output = tokenizer.decode(generated_ids[0][len(input_ids[0]):], skip_special_tokens=True).strip()
        reference = tokenizer.decode([i for i in label_ids if i != -100], skip_special_tokens=True).strip()

        predictions.append(output)
        references.append([reference])

        # ROUGE & BLEU
        r = scorer.score(reference, output)
        rouge1s.append(r['rouge1'].fmeasure)
        rougeLs.append(r['rougeL'].fmeasure)
        bleus.append(sentence_bleu([reference.split()], output.split(), smoothing_function=smoothing))

        # Perplexity
        input = tokenizer(reference, return_tensors="pt").input_ids.to(device)
        with torch.no_grad():
            loss = model(input, labels=input).loss
            ppl = torch.exp(loss).item()
            perplexities.append(ppl)
        print(f"Perplexity: {ppl}")
        print(f"BLEU: {sentence_bleu([reference.split()], output.split(), smoothing_function=smoothing)}")
        print(f"ROUGE-1: {r['rouge1'].fmeasure}")
        print(f"ROUGE-L: {r['rougeL'].fmeasure}")
        print(f"Output: {output}")
        print(f"Reference: {reference}")

print("\n--- Evaluation Report ---")
print(f"Average ROUGE-1: {np.mean(rouge1s):.4f}")
print(f"Average ROUGE-L: {np.mean(rougeLs):.4f}")
print(f"Average BLEU:     {np.mean(bleus):.4f}")
print(f"Average Perplexity: {np.mean(perplexities):.2f}")

