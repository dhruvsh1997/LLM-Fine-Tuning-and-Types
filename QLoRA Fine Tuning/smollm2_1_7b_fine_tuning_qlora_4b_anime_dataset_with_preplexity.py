# -*- coding: utf-8 -*-
"""SmolLM2_1_7B_Fine_Tuning_QLoRA_4b_Anime_Dataset_with_Preplexity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1swXCz5YngvZhZmeze3EpqFz1VEqi36v4
"""

# ---------DEPENDENCIES-----------
!pip install -q transformers accelerate datasets peft bitsandbytes trl evaluate rouge_score nltk polars

# ------------QLoRA Fine-Tuning for Anime QA Dataset with Perplexity, ROUGE, BLEU Evaluation-----------
import os
import torch
import math
import json
import polars as pl
import numpy as np
import logging
from datasets import Dataset
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling
)
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk import download
from typing import List
# ---------NLTK SETUP-----------
try:
    download('punkt')
except:
    pass
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger("qlora-anime-qa")

# ---------DEVICE SETUP-----------
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Device: {device}")

# ---------LOAD TOKENIZER & MODEL (QLoRA READY)-----------
model_name = "HuggingFaceTB/SmolLM-1.7B-Instruct"

# Quantization Guide:
# Use one of the following configurations depending on your precision/efficiency goals:
#
# 4-bit Quantization (QLoRA): Use NF4 quant type with bfloat16 compute for best performance
# To switch to 8-bit, use `load_in_8bit=True` in quantization_config instead
# For pure FP16 or BF16, skip quantization_config and set torch_dtype + bf16=True in TrainingArguments
# 4-bit QLoRA (Recommended for memory efficiency + training performance):
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)
#
# 8-bit Quantization (Good for inference-only memory savings):
#     bnb_config = BitsAndBytesConfig(
#         load_in_8bit=True
#     )
#
# No quantization (Full precision FP16/BF16 training):
#     # Remove `quantization_config` entirely
#     # Instead, use:
#     torch_dtype=torch.bfloat16 or torch.float16
#     bf16=True (in TrainingArguments)

tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16
)

base_model = prepare_model_for_kbit_training(base_model)

# ---------LORA CONFIG-----------
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(base_model, lora_config)
model.print_trainable_parameters()

# ---------LOAD ANIME DATASET-----------
anime_list = [
    "aot", "naruto", "onepiece", "hellsing", "dr_stone", "berserk", "evangelion",
    "darling-in-the-franxx", "frieren", "gundam_00", "kurokonobasuke", "chainsawman", "onepunch"
]

formatted_examples = []
for anime in anime_list:
    path = f"hf://datasets/theblackcat102/anime-understanding-dataset/{anime}_dev.jsonl"
    df = pl.read_ndjson(path).to_pandas()
    for row in df.to_dict(orient="records"):
        correct = row[row["answer"]]
        messages = [
            {"role": "user", "content": f"Answer this question about {anime} anime:\n\n{row['question']}\n\nA) {row['A']}\nB) {row['B']}\nC) {row['C']}\nD) {row['D']}"},
            {"role": "assistant", "content": f"The correct answer is {row['answer']}) {correct}"}
        ]
        text = tokenizer.apply_chat_template(messages, tokenize=False)
        formatted_examples.append({"text": text, "correct_letter": row["answer"], "answer": correct})

dataset = Dataset.from_list(formatted_examples)

# ---------TOKENIZE DATA-----------
def tokenize(example):
    return tokenizer(example["text"], truncation=True, padding=False, max_length=512)

tokenized = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)

collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, pad_to_multiple_of=8)

# ---------TRAINING CONFIG-----------
args = TrainingArguments(
    output_dir="qlora_anime_qa_model",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    num_train_epochs=15,
    learning_rate=2e-4,
    fp16=False,
    bf16=True,
    logging_steps=10,
    save_strategy="no",
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized,
    data_collator=collator,
    compute_metrics=None,
    preprocess_logits_for_metrics=None,
    tokenizer=None,  # Donâ€™t pass tokenizer to avoid tokenizer deprecation
)
trainer.label_names = ["labels"]  # Explicitly set to suppress warning

trainer.train()

# ---------SAVE MODEL-----------
model.save_pretrained("qlora_anime_qa_model")
tokenizer.save_pretrained("qlora_anime_qa_model")

# ---------EVALUATION METRICS-----------
class PerplexityEvaluator:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def compute(self, prompt, target):
        full = prompt + " " + target
        full_ids = self.tokenizer(full, return_tensors="pt").input_ids.to(self.device)
        prompt_ids = self.tokenizer(prompt, return_tensors="pt").input_ids.to(self.device)
        context_len = prompt_ids.shape[1]
        with torch.no_grad():
            out = self.model(full_ids, labels=full_ids)
            logits = out.logits[0, context_len-1:-1]
            target_ids = full_ids[0, context_len:]
            log_probs = torch.nn.functional.log_softmax(logits, dim=-1)
            selected = log_probs.gather(1, target_ids.unsqueeze(1)).squeeze(1)
            return torch.exp(-selected.mean()).item()

scorer = rouge_scorer.RougeScorer(["rouge1", "rougeL"], use_stemmer=True)
smooth = SmoothingFunction().method1

eval_model = model.merge_and_unload()
eval_model.eval()
evaluator = PerplexityEvaluator(eval_model, tokenizer, device)

examples = dataset.select(range(15))
rouge_1s, rouge_Ls, bleus, perplexities = [], [], [], []

for ex in examples:
    text = ex["text"]
    parts = text.split("<|im_start|>assistant\n")
    if len(parts) != 2:
        continue
    prompt = parts[0] + "<|im_start|>assistant\n"
    answer = parts[1].replace("<|im_end|>", "").strip()

    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)
    gen_ids = eval_model.generate(input_ids, max_new_tokens=60, pad_token_id=tokenizer.eos_token_id)
    output = tokenizer.decode(gen_ids[0][input_ids.shape[1]:], skip_special_tokens=True).strip()

    ppl = evaluator.compute(prompt, answer)
    perplexities.append(ppl)

    rouge = scorer.score(answer, output)
    rouge_1s.append(rouge["rouge1"].fmeasure)
    rouge_Ls.append(rouge["rougeL"].fmeasure)

    bleus.append(sentence_bleu([answer.split()], output.split(), smoothing_function=smooth))
    print(f"Prompt: {prompt}")
    print(f"Answer: {answer}")
    print(f"Output: {output}")
    print(f"Perplexity: {ppl:.2f}")
    print(f"ROUGE-1: {rouge['rouge1'].fmeasure:.3f}")
    print(f"ROUGE-L: {rouge['rougeL'].fmeasure:.3f}")
    print(f"BLEU: {sentence_bleu([answer.split()], output.split(), smoothing_function=smooth):.3f}")
    print("----------------------------------")

print("--- Evaluation Summary ---")
print(f"Average Perplexity: {np.mean(perplexities):.2f}")
print(f"Average ROUGE-1: {np.mean(rouge_1s):.3f}")
print(f"Average ROUGE-L: {np.mean(rouge_Ls):.3f}")
print(f"Average BLEU: {np.mean(bleus):.3f}")

